{"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import argparse\nimport os\nfrom functools import partial\n\nimport numpy as np\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport torch.nn.parallel\nimport torch.utils.data.distributed\nfrom trainer import run_training\nfrom monai.inferers import sliding_window_inference\nfrom monai.losses import DiceCELoss, FocalLoss\nfrom monai.metrics import DiceMetric, MeanIoU\nfrom monai.transforms import Activations, AsDiscrete, Compose\nfrom monai.utils.enums import MetricReduction\nfrom monai.visualize import matshow3d\n\n# from utils.myModel import MyModel, MyModel2d, MyModel3dunet, MyFlexibleUNet2d, MyFlexibleUNet2dLSTM, MyBasicUNetPlusPlus\n\nfrom pathlib import Path\nfrom easydict import EasyDict as edict\nTEST_DIR = Path('/root/autodl-tmp/vesuvius-challenge-ink-detection/test')","metadata":{},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# args = edict(RandFlipd_prob=0.2, RandRotate90d_prob=0.2, RandScaleIntensityd_prob=0.1, RandShiftIntensityd_prob=0.1, a_max=65535.0, a_min=0.0, amp=True, b_max=255.0, b_min=0.0, batch_size=1, cache_rate=1.0, checkpoint=None, data_dir='/root/autodl-tmp/vesuvius-challenge-ink-detection', debug=False, dist_backend='nccl', dist_url='tcp://127.0.0.1:23456', distributed=False, dropout_path_rate=0.0, dropout_rate=0.0, eff='b3', feature_size=48, gpu=0, in_channels=65, infer_overlap=0.5, json_list='/root/autodl-tmp/data_split/data_split.json', logdir='./runs/512_funetlstm_b3_16_sgd_continue', loss_mode='custom', loss_weight=(2.0, 1.0), lrschedule='cosine_anneal', max_epochs=2000, mid=28, model_mode='2dfunetlstm', momentum=0.99, noamp=False, norm_name='instance', normal=False, num_channel=16, num_samples=4, optim_lr=0.0001, optim_name='sgd', out_channels=1, pretrained_dir='./pretrained_models/', pretrained_model_name='512_funetlstm_b3_16_sgd_1000.pt', rank=0, reg_weight=1e-05, resume_ckpt=True, roi_x=512, roi_y=512, roi_z=16, save_checkpoint=True, smooth_dr=1e-06, smooth_nr=0.0, space_x=1.5, space_y=1.5, space_z=1.0, spatial_dims=3, sw_batch_size=4, test_mode=False, threshold=0.4, use_checkpoint=False, use_normal_dataset=False, use_ssl_pretrained=False, val_every=10, warmup_epochs=50, workers=0, world_size=1)\nargs = edict(RandFlipd_prob=0.2, RandRotate90d_prob=0.2, RandScaleIntensityd_prob=0.1, RandShiftIntensityd_prob=0.1, a_max=65535.0, a_min=0.0, amp=True, b_max=255.0, b_min=0.0, \n             batch_size=1, cache_rate=1.0, checkpoint=None, data_dir='/root/autodl-tmp/vesuvius-challenge-ink-detection', debug=False, dist_backend='nccl', dist_url='tcp://127.0.0.1:23456', \n             distributed=False, dropout_path_rate=0.0, dropout_rate=0.0, eff='b3', feature_size=48, gpu=0, in_channels=65, infer_overlap=0.5, json_list='/root/autodl-tmp/data_split/data_split.json', \n             logdir='./runs/512_funetlstm_b3_16_sgd_continue_1400', loss_mode='custom', loss_weight=(2.0, 1.0), lrschedule='cosine_anneal', max_epochs=1000, mid=26, model_mode='2dfunetlstm', \n             momentum=0.99, noamp=False, norm_name='instance', normal=False, num_channel=16, num_samples=4, optim_lr=0.0005, optim_name='adamw', out_channels=1, pretrained_dir='./pretrained_models/', \n             pretrained_model_name='512_funetlstm_b3_16_sgd_1400.pt', rank=0, reg_weight=1e-05, resume_ckpt=True, roi_x=512, roi_y=512, roi_z=16, save_checkpoint=True, smooth_dr=1e-06, smooth_nr=0.0, \n             space_x=1.5, space_y=1.5, space_z=1.0, spatial_dims=3, sw_batch_size=4, test_mode=False, threshold=0.4, use_checkpoint=False, use_normal_dataset=False, use_ssl_pretrained=False, \n             val_every=10, warmup_epochs=50, workers=0, world_size=1)\n","metadata":{},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Model\n- MyModel\n- MyModel2d\n- MyModel3dunet\n- MyFlexibleUNet2d\n- MyFlexibleUNet2dLSTM \n- MyBasicUNetPlusPlus","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nfrom monai.networks.nets import SwinUNETR, UNet, FlexibleUNet, BasicUNetPlusPlus\nfrom monai.networks.blocks.convolutions import Convolution\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = SwinUNETR(\n            img_size=(96,96,96),\n            in_channels=1,\n            out_channels=14,\n            feature_size=48,\n            drop_rate=0.0,\n            attn_drop_rate=0.0,\n            dropout_path_rate=0.0,\n            use_checkpoint=True,\n        )\n        # self.conv1 = Convolution(spatial_dims=3, in_channels=14, out_channels=1, kernel_size=1)\n        self.conv2 = Convolution(spatial_dims=3, in_channels=1, out_channels=1, kernel_size=(1, 1, 64), strides=1, padding=0, act=\"sigmoid\")\n\n    \n    def forward(self, x):\n        if x[0].size() != (1, 64, 64, 64):\n            print(x.size())\n            raise ValueError(\"Input size is not correct\")\n        x_out = self.swinUNETR(x)\n        # x_out = self.conv1(x_out)\n        x_out = self.conv2(x_out)\n        return x_out\n    \n    def load_swin_ckpt(self, model_dict, strict: bool = True):\n        self.swinUNETR.load_state_dict(model_dict, strict)\n        pass\n    \nclass MyModel2d(nn.Module):\n    def __init__(self,img_size=(192, 192)):\n        super().__init__()\n        self.swinUNETR = SwinUNETR(\n                                img_size=img_size,\n                                in_channels=65,\n                                out_channels=1,\n                                feature_size=12,\n                                use_checkpoint=True, \n                                spatial_dims=2\n                                )\n\n    \n    def forward(self, x):\n        x_out = self.swinUNETR(x)\n        return x_out\n    \n    def load_swin_ckpt(self, model_dict, strict: bool = True):\n        self.swinUNETR.load_state_dict(model_dict, strict)\n        pass\n    \nclass MyModel3dunet(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.unet = UNet(\n            spatial_dims=3,\n            in_channels=1,\n            out_channels=1,\n            channels=(16, 32, 64, 128, 256),\n            strides=(2, 2, 2, 2),\n            num_res_units=2,\n        )\n        self.conv1 = Convolution(spatial_dims=3, in_channels=1, out_channels=1, kernel_size=(1, 1, 64), strides=1, padding=0, act=\"sigmoid\")\n    \n    def forward(self, x):\n        x_out = self.unet(x)\n        x_out = self.conv1(x_out)\n        return x_out\n    \n\nclass MyFlexibleUNet2d(nn.Module):\n    def __init__(self, args) -> None:\n        super().__init__()\n        self.flexibleUNet = FlexibleUNet(\n                in_channels=args.num_channel,\n                out_channels=1,\n                backbone=f\"efficientnet-{args.eff}\",\n                pretrained=True,\n                spatial_dims=2,\n                dropout=0.0,\n            )\n        self.sig = nn.Sigmoid()\n        \n    def forward(self, x):\n        x_out = self.flexibleUNet(x)\n        x_out = self.sig(x_out)\n        return x_out\n        \n        \n\nclass ConvLSTM(nn.Module):\n    def __init__(self, in_channels=320, out_channels=320, kernel_size=1, padding=0, batch_first=True):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)\n        self.lstm = nn.LSTM(256, 256, batch_first=batch_first)\n\n    def forward(self, x):\n\n        last_feature = x[-1]\n\n        # print(\"Before modification:\")\n        # print(x[-1][0, 0, 0, :2])  # print a small part of the tensor\n        # print(last_feature.shape)\n        batch_size, channels, height, width = last_feature.shape\n\n        # Apply 2D convolution\n        last_feature = self.conv(last_feature)\n\n        # Reshape output for LSTM\n        last_feature = last_feature.view(batch_size, -1, height * width)\n\n        # Pass through LSTM\n        lstm_out, _ = self.lstm(last_feature)\n\n        # Reshape output back to original shape\n        last_feature = lstm_out.view(batch_size, channels, height, width)\n\n        x[-1] = last_feature\n\n        # print(\"After modification:\")\n        # print(x[-1][0, 0, 0, :2])  # print a small part of the tensor\n\n        return x\n\n\nclass MyFlexibleUNet2dLSTM(nn.Module):\n    def __init__(self, args):\n        super().__init__()\n        self.flexibleUNet = FlexibleUNet(\n            in_channels=args.num_channel,\n            out_channels=1,\n            backbone=\"efficientnet-b0\",\n            pretrained=True,\n            spatial_dims=2,\n            dropout=0.0,\n        )\n        # Add ConvLSTM layer after the last convolution layer in the encoder\n        assert args.roi_x == args.roi_y, \"ROI x and y must be the same\"\n        self.conv_lstm = ConvLSTM()\n        self.sig = nn.Sigmoid()\n\n    def forward(self, x):\n        x_out = self.flexibleUNet.encoder(x)\n        x_out = self.conv_lstm(x_out)\n        x_out = self.flexibleUNet.decoder(x_out)\n        x_out = self.flexibleUNet.segmentation_head(x_out)\n        x_out = self.sig(x_out)\n        return x_out\n    \nclass MyBasicUNetPlusPlus(nn.Module):\n    def __init__(self, args) -> None:\n        super().__init__()\n        self.basicUNetPlusPlus = BasicUNetPlusPlus(in_channels=1, out_channels=1)\n        \n    def forward(self, x):\n        x_out = self.basicUNetPlusPlus(x)\n        return x_out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# utils\n\n- def resample_3d\n- def resample_2d\n- class Sampler","metadata":{}},{"cell_type":"code","source":"def resample_3d(img, target_size):\n    imx, imy, imz = img.shape\n    tx, ty, tz = target_size\n    zoom_ratio = (float(tx) / float(imx), float(ty) / float(imy), float(tz) / float(imz))\n    img_resampled = ndimage.zoom(img, zoom_ratio, order=0, prefilter=False)\n    return img_resampled\n\ndef resample_2d(img, target_size):\n    imx, imy = img.shape\n    tx, ty = target_size\n    zoom_ratio = (float(tx) / float(imx), float(ty) / float(imy))\n    img_resampled = ndimage.zoom(img, zoom_ratio, order=0, prefilter=False)\n    return img_resampled\n\nclass Sampler(torch.utils.data.Sampler):\n    def __init__(self, dataset, num_replicas=None, rank=None, shuffle=True, make_even=True):\n        if num_replicas is None:\n            if not torch.distributed.is_available():\n                raise RuntimeError(\n                    \"Requires distributed package to be available\")\n            num_replicas = torch.distributed.get_world_size()\n        if rank is None:\n            if not torch.distributed.is_available():\n                raise RuntimeError(\n                    \"Requires distributed package to be available\")\n            rank = torch.distributed.get_rank()\n        self.shuffle = shuffle\n        self.make_even = make_even\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.num_samples = int(\n            math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n        indices = list(range(len(self.dataset)))\n        self.valid_length = len(\n            indices[self.rank: self.total_size: self.num_replicas])\n\n    def __iter__(self):\n        if self.shuffle:\n            g = torch.Generator()\n            g.manual_seed(self.epoch)\n            indices = torch.randperm(len(self.dataset), generator=g).tolist()\n        else:\n            indices = list(range(len(self.dataset)))\n        if self.make_even:\n            if len(indices) < self.total_size:\n                if self.total_size - len(indices) < len(indices):\n                    indices += indices[: (self.total_size - len(indices))]\n                else:\n                    extra_ids = np.random.randint(low=0, high=len(\n                        indices), size=self.total_size - len(indices))\n                    indices += [indices[ids] for ids in extra_ids]\n            assert len(indices) == self.total_size\n        indices = indices[self.rank: self.total_size: self.num_replicas]\n        self.num_samples = len(indices)\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom Transform","metadata":{}},{"cell_type":"code","source":"class Copy(Transform):\n    def __init__(self, num_channel, add_channel=False):\n        self.num_channel = num_channel\n        self.add_channel = add_channel\n\n    def __call__(self, data):\n        if self.add_channel:\n            data = data.repeat(1, self.num_channel, 1, 1)  # output = (batch_size=1, num_channel, H, W)\n        else:\n            data = data.repeat(self.num_channel, 1, 1)  # output = (batch_size=1, num_channel, H, W)\n        return data\n    \nclass Copyd(MapTransform):\n    \"\"\"\n    Dictionary-based wrapper of :py:class:`monai.transforms.AddChannel`.\n    \"\"\"\n    def __init__(self, keys: KeysCollection, num_channel, add_channel=False) -> None:\n        \"\"\"\n        Args:\n            keys: keys of the corresponding items to be transformed.\n                See also: :py:class:`monai.transforms.compose.MapTransform`\n            allow_missing_keys: don't raise exception if key is missing.\n        \"\"\"\n        super().__init__(keys, )\n        self.adder = Copy(num_channel, add_channel)\n\n    def __call__(self, data: Mapping[Hashable, NdarrayOrTensor]) -> Dict[Hashable, NdarrayOrTensor]:\n        d = dict(data)\n        for key in self.key_iterator(d):\n            d[key] = self.adder(d[key])\n        return d","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from monai.data import load_decathlon_datalist\nfrom monai import data, transforms\n\ndef get_loader(args):\n    data_dir = args.data_dir\n    datalist_json = os.path.join(data_dir, args.json_list)\n    test_transform = transforms.Compose(\n        [\n            transforms.LoadImaged(\n                keys=[\"image\", \"label\"], reader=\"NumpyReader\"),\n            Copyd(keys=[\"label\"],\n                    num_channel=args.num_channel),\n            transforms.AddChanneld(keys=[\"image\", 'label']),\n            # transforms.CropForegroundd(\n            #     keys=[\"image\", \"label\"], source_key=\"image\"),\n            # transforms.Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n            # change_channeld(keys=[\"image\", \"label\", 'inklabels']),\n            # transforms.Spacingd(keys=\"image\", pixdim=(args.space_x, args.space_y, args.space_z), mode=\"bilinear\"),\n            transforms.ScaleIntensityRanged(\n                keys=[\"image\"], a_min=args.a_min, a_max=args.a_max, b_min=args.b_min, b_max=args.b_max, clip=True\n            ),\n            transforms.ToTensord(keys=[\"image\"]),\n        ]\n    )\n    val_files = load_decathlon_datalist(\n        datalist_json, True, \"testing\", base_dir=data_dir)\n    val_ds = data.Dataset(data=val_files, transform=test_transform)\n    val_sampler = Sampler(\n        val_ds, shuffle=False) if args.distributed else None\n    val_loader = data.DataLoader(\n        val_ds, batch_size=8, shuffle=False, num_workers=args.workers, sampler=val_sampler, pin_memory=True\n    )\n    loader = val_loader\n    return loader","metadata":{},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# Test Function","metadata":{}},{"cell_type":"code","source":"# from utils.utils import resample_3d, resample_2d\n\ndef test(model_infer, val_loader, args):\n    output_directory = Path(\"/kaggle/working/\") / args.exp_name\n    output_directory.mkdir(parents=True, exist_ok=True)\n    with torch.no_grad():\n        dice_list_case = []\n        for i, batch in enumerate(val_loader):\n            val_inputs, val_labels = (batch[\"image\"].cuda(), batch[\"label\"].cuda())\n            print(type(val_labels))\n            print(val_labels.shape)\n            _, d, h, w = val_labels.shape\n            target_shape = (h, w)\n            img_name = batch[\"image_meta_dict\"][\"filename_or_obj\"][0].split(\"/\")[-1]\n            print(\"Inference on case {}\".format(img_name))\n            val_outputs = model_infer(val_inputs)\n            print(val_outputs.shape)\n            val_outputs = torch.softmax(val_outputs, 1).cpu()\n            val_outputs = np.array(val_outputs)\n            val_outputs = np.argmax(val_outputs, axis=1).astype(np.uint8)[0]\n            val_labels = val_labels.cpu()\n            val_labels = np.array(val_labels)[0, 0, :, :]\n            if args.model_mode in [\"2dswin\", \"2dfunetlstm\"]:\n                val_outputs = resample_2d(val_outputs, target_shape)\n            elif args.model_mode == \"3dswin\":\n                val_outputs = resample_3d(val_outputs, target_shape)\n            else:\n                raise ValueError(\"model_mode should be ['3dswin', '2dswin', '3dunet', '2dunet']\")\n\n            np.save(\n                os.path.join(output_directory, img_name), val_outputs[:,:]\n            )\n    return None","metadata":{},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"np.set_printoptions(formatter={\"float\": \"{: 0.3f}\".format}, suppress=True)\n\ntorch.cuda.set_device(0)\ntorch.backends.cudnn.benchmark = True\nargs.test_mode = True\nloader = get_loader(args)\n\ninf_size = [args.roi_x, args.roi_y, args.roi_z]\n\npretrained_dir = args.pretrained_dir\nif args.model_mode == \"3dswin\":\n    model = MyModel(img_size=(args.roi_x,args.roi_y,args.roi_y))\nelif args.model_mode == \"2dswin\":\n    model = MyModel2d(img_size=(args.roi_x,args.roi_y))\nelif args.model_mode == \"3dunet\":\n    model = MyModel3dunet()\nelif args.model_mode == \"2dfunet\":\n    model = MyFlexibleUNet2d(args)\nelif args.model_mode == \"2dfunetlstm\":\n    model = MyFlexibleUNet2dLSTM(args)\nelif args.model_mode == \"3dunet++\":\n    model = MyBasicUNetPlusPlus(args)\nelse:\n    raise ValueError(\"model mode error\")\n\n\nmodel_dict = torch.load(os.path.join(pretrained_dir, args.pretrained_model_name))[\"state_dict\"]\nif args.model_mode in [\"2dswin\", \"3dunet\", \"2dfunet\", \"2dfunetlstm\", \"3dunet++\"]:\n    model.load_state_dict(model_dict)\nelif args.model_mode == \"3dswin\":\n    model.load_swin_ckpt(model_dict)\nelse:\n    raise ValueError(\"model mode error\")\n\nif args.model_mode in [\"3dswin\", \"3dunet\", \"3dunet++\"]:\n    model_inferer = partial(\n        sliding_window_inference,\n        roi_size = (args.roi_x,args.roi_y,args.roi_z),\n        sw_batch_size = 8,\n        predictor = model,\n        overlap = 0.5,\n        progress = True,\n        padding_mode = \"reflect\", \n        device = \"cpu\", \n        sw_device = \"cuda\"\n    )\nelif args.model_mode in [\"2dswin\", \"2dfunet\", \"2dfunetlstm\"]:\n    model_inferer = partial(\n        sliding_window_inference,\n        roi_size = (args.roi_x,args.roi_y),\n        sw_batch_size = 8,\n        predictor = model,\n        overlap = 0.5,\n        progress = True,\n        padding_mode = \"reflect\", \n        device = \"cpu\", \n        sw_device = \"cuda\"\n    )     \nelse:\n    raise ValueError(\"model mode error\")\n\nmodel.cuda(0)\n\nprint(args)\ntest(model_inferer, loader, args)\n","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":30,"outputs":[{"name":"stdout","output_type":"stream","text":"Total parameters count 8160661\n\nLodaer test\n\n2023-04-24 12:39:09,082 - > collate dict key \"image\" out of 4 keys\n\n2023-04-24 12:39:09,088 - >> collate/stack a list of tensors\n\n2023-04-24 12:39:09,094 - >> E: stack expects each tensor to be equal size, but got [1, 65, 2727, 6330] at entry 0 and [1, 65, 5454, 6330] at entry 1, shape [(1, 65, 2727, 6330), (1, 65, 5454, 6330)] in collate([tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          ...,\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          ...,\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          ...,\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n\n\n         ...,\n\n\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          ...,\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          ...,\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          ...,\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n\nMetadata\n\n\tspatial_shape: [  65 2727 6330]\n\n\tspace: RAS\n\n\toriginal_channel_dim: no_channel\n\n\tfilename_or_obj: /root/autodl-tmp/vesuvius-challenge-ink-detection/test/a/surface_volume.npy\n\n\taffine: tensor([[1., 0., 0., 0.],\n\n        [0., 1., 0., 0.],\n\n        [0., 0., 1., 0.],\n\n        [0., 0., 0., 1.]], dtype=torch.float64)\n\n\n\nApplied operations\n\n[]\n\nIs batch?: False, tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          ...,\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          ...,\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          ...,\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n\n\n         ...,\n\n\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          ...,\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          ...,\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          ...,\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n\nMetadata\n\n\tspatial_shape: [  65 5454 6330]\n\n\tspace: RAS\n\n\toriginal_channel_dim: no_channel\n\n\tfilename_or_obj: /root/autodl-tmp/vesuvius-challenge-ink-detection/test/b/surface_volume.npy\n\n\taffine: tensor([[1., 0., 0., 0.],\n\n        [0., 1., 0., 0.],\n\n        [0., 0., 1., 0.],\n\n        [0., 0., 0., 1.]], dtype=torch.float64)\n\n\n\nApplied operations\n\n[]\n\nIs batch?: False])\n\n2023-04-24 12:39:09,095 - > collate dict key \"label\" out of 4 keys\n\n2023-04-24 12:39:09,100 - >> collate/stack a list of tensors\n\n2023-04-24 12:39:09,108 - >> E: stack expects each tensor to be equal size, but got [1, 16, 2727, 6330] at entry 0 and [1, 16, 5454, 6330] at entry 1, shape [(1, 16, 2727, 6330), (1, 16, 5454, 6330)] in collate([tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          ...,\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          ...,\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          ...,\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n\n\n         ...,\n\n\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          ...,\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          ...,\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          ...,\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n\nMetadata\n\n\tspatial_shape: [2727 6330]\n\n\tspace: RAS\n\n\toriginal_channel_dim: no_channel\n\n\tfilename_or_obj: /root/autodl-tmp/vesuvius-challenge-ink-detection/test/a/mask.npy\n\n\taffine: tensor([[1., 0., 0., 0.],\n\n        [0., 1., 0., 0.],\n\n        [0., 0., 1., 0.],\n\n        [0., 0., 0., 1.]], dtype=torch.float64)\n\n\n\nApplied operations\n\n[]\n\nIs batch?: False, tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          ...,\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          ...,\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          ...,\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n\n\n         ...,\n\n\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          ...,\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          ...,\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          ...,\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.],\n\n          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n\nMetadata\n\n\tspatial_shape: [5454 6330]\n\n\tspace: RAS\n\n\toriginal_channel_dim: no_channel\n\n\tfilename_or_obj: /root/autodl-tmp/vesuvius-challenge-ink-detection/test/b/mask.npy\n\n\taffine: tensor([[1., 0., 0., 0.],\n\n        [0., 1., 0., 0.],\n\n        [0., 0., 1., 0.],\n\n        [0., 0., 0., 1.]], dtype=torch.float64)\n\n\n\nApplied operations\n\n[]\n\nIs batch?: False])\n\n2023-04-24 12:39:09,109 - > collate dict key \"image_meta_dict\" out of 4 keys\n\n2023-04-24 12:39:09,111 - >> collate dict key \"spatial_shape\" out of 5 keys\n\n2023-04-24 12:39:09,111 - >>> collate/stack a list of numpy arrays\n\n2023-04-24 12:39:09,113 - >>> collate/stack a list of tensors\n\n2023-04-24 12:39:09,113 - >> collate dict key \"space\" out of 5 keys\n\n2023-04-24 12:39:09,114 - >> collate dict key \"original_channel_dim\" out of 5 keys\n\n2023-04-24 12:39:09,114 - >> collate dict key \"filename_or_obj\" out of 5 keys\n\n2023-04-24 12:39:09,115 - >> collate dict key \"affine\" out of 5 keys\n\n2023-04-24 12:39:09,118 - >>> collate/stack a list of tensors\n\n2023-04-24 12:39:09,119 - > collate dict key \"label_meta_dict\" out of 4 keys\n\n2023-04-24 12:39:09,122 - >> collate dict key \"spatial_shape\" out of 5 keys\n\n2023-04-24 12:39:09,123 - >>> collate/stack a list of numpy arrays\n\n2023-04-24 12:39:09,124 - >>> collate/stack a list of tensors\n\n2023-04-24 12:39:09,125 - >> collate dict key \"space\" out of 5 keys\n\n2023-04-24 12:39:09,126 - >> collate dict key \"original_channel_dim\" out of 5 keys\n\n2023-04-24 12:39:09,126 - >> collate dict key \"filename_or_obj\" out of 5 keys\n\n2023-04-24 12:39:09,127 - >> collate dict key \"affine\" out of 5 keys\n\n2023-04-24 12:39:09,129 - >>> collate/stack a list of tensors\n"},{"ename":"RuntimeError","evalue":"stack expects each tensor to be equal size, but got [1, 65, 2727, 6330] at entry 0 and [1, 65, 5454, 6330] at entry 1\nCollate error on the key 'image' of dictionary data.\n\nMONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation).","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/monai/data/utils.py\u001b[0m in \u001b[0;36mlist_data_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    461\u001b[0m                 \u001b[0mdata_for_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m                 \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollate_meta_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_for_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/monai/data/utils.py\u001b[0m in \u001b[0;36mcollate_meta_tensor\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMetaObj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0mcollated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0mcollated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTraceKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNONE\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/monai/data/meta_tensor.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__torch_function__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0;31m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisableTorchFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_default_nowrap_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 65, 2727, 6330] at entry 0 and [1, 65, 5454, 6330] at entry 1","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_155287/335093777.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Lodaer test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/monai/data/utils.py\u001b[0m in \u001b[0;36mlist_data_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    475\u001b[0m             )\n\u001b[1;32m    476\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0mre_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 65, 2727, 6330] at entry 0 and [1, 65, 5454, 6330] at entry 1\nCollate error on the key 'image' of dictionary data.\n\nMONAI hint: if your transforms intentionally create images of different shapes, creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its documentation)."]}]}]}